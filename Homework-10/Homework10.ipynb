{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgKcWatuRSeF",
        "outputId": "4db384dc-a4eb-4139-b3da-320fe5e9bb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8deca2",
        "outputId": "d30583db-0057-46f7-e510-155144eb6bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJllN1dYNhNL",
        "outputId": "52aa1115-ce9e-42fa-ca1d-0f1d5f0a3014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApGnTZYI5-EF",
        "outputId": "6837a1ad-6294-428f-cd6f-a75cbf9a827c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2e9Ja_caPOm",
        "outputId": "c64007bc-1a72-43ed-e8e4-5d5a760db7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1 - Glove Question"
      ],
      "metadata": {
        "id": "kyQFKq-6Cgep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential\n",
        "!pip install Cython\n",
        "!pip install glove-python3\n",
        "!pip install glove-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqp5Um_UCiBN",
        "outputId": "4ff81a28-e86f-4f00-e070-e0c67d939183"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (3.0.12)\n",
            "Collecting glove-python3\n",
            "  Downloading glove_python3-0.1.0.tar.gz (326 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.0/327.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from glove-python3) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from glove-python3) (1.16.3)\n",
            "Building wheels for collected packages: glove-python3\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for glove-python3 (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for glove-python3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for glove-python3\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for glove-python3\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build glove-python3\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (glove-python3)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting glove-py\n",
            "  Downloading glove_py-0.2.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from glove-py)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from glove-py) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from glove-py) (2.0.2)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Building wheels for collected packages: glove-py\n",
            "  Building wheel for glove-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-py: filename=glove_py-0.2.3-cp312-cp312-linux_x86_64.whl size=1245922 sha256=6372e091d7e0359a59ea25bcb76e56334e53ced2a8c9dfe1cf65a70bf948194b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/62/cd/dc7b15fcc4c9853762aab01cf9ec89de6f393ecc94ce629754\n",
            "Successfully built glove-py\n",
            "Installing collected packages: pybind11, glove-py\n",
            "Successfully installed glove-py-0.2.3 pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(filepath):\n",
        "    \"\"\"Loads GloVe embeddings from a file.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Specify the path to your GloVe embedding file\n",
        "glove_filepath = '/content/drive/MyDrive/KDD/Homework10/glove.6B/glove.6B.50d.txt'\n",
        "\n"
      ],
      "metadata": {
        "id": "mEmRWAm6CuHT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the embeddings\n",
        "print(f\"Loading GloVe embeddings from {glove_filepath}...\")\n",
        "# This loads the entire GloVe embedding dictionary, not just for the specified words\n",
        "glove_embeddings = load_glove_embeddings(glove_filepath)\n",
        "print(\"Embeddings loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3dm1QnOCy15",
        "outputId": "a4725670-d489-49c1-8f01-d71499962d53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings from /content/drive/MyDrive/KDD/Homework10/glove.6B/glove.6B.50d.txt...\n",
            "Embeddings loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8fNA_4lC5QM",
        "outputId": "fdde517b-9d1e-4e53-ea82-404c434618b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define phrase and tokenize put in lowercase words for \"I love deep learning and text mining\"\n",
        "phrase = \"I love deep learning and text mining\"\n",
        "words = nltk.word_tokenize(phrase.lower())\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3YfqHTqC55j",
        "outputId": "15c3cb19-d2af-40f3-f015-972af3cc840a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'deep', 'learning', 'and', 'text', 'mining']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vocabulary\n",
        "vocab = {'<pad>': 0}\n",
        "index =1\n",
        "for word in words:\n",
        "  if word not in vocab:\n",
        "    vocab[word] = index\n",
        "    index +=1\n",
        "print(\"Vocabulary:\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWofoUfTC8Tn",
        "outputId": "11d1a3c2-d5e3-4671-d74b-009152bb0935"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'<pad>': 0, 'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'and': 5, 'text': 6, 'mining': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to retrieve word embeddings\n",
        "def get_word_embedding(word):\n",
        "  if word in vocab:\n",
        "    return glove_embeddings[word]\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "ng9g97-jDBf6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word Embeddings:\")\n",
        "for word in words:\n",
        "  embedding = get_word_embedding(word)\n",
        "  if embedding is not None:\n",
        "    print(f\"{word}: {embedding}\")\n",
        "  else:\n",
        "    print(f\"{word} not found in embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTEewtCbDCjR",
        "outputId": "eb3b8a75-0eb6-461b-b881-574558a54b85"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embeddings:\n",
            "i: [ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
            " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
            " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
            " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
            "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
            "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
            "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
            " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
            " -2.6671e-01  9.2121e-01]\n",
            "love: [-0.13886    1.1401    -0.85212   -0.29212    0.75534    0.82762\n",
            " -0.3181     0.0072204 -0.34762    1.0731    -0.24665    0.97765\n",
            " -0.55835   -0.090318   0.83182   -0.33317    0.22648    0.30913\n",
            "  0.026929  -0.086739  -0.14703    1.3543     0.53695    0.43735\n",
            "  1.2749    -1.4382    -1.2815    -0.15196    1.0506    -0.93644\n",
            "  2.7561     0.58967   -0.29473    0.27574   -0.32928   -0.201\n",
            " -0.28547   -0.45987   -0.14603   -0.69372    0.070761  -0.19326\n",
            " -0.1855    -0.16095    0.24268    0.20784    0.030924  -1.3711\n",
            " -0.28606    0.2898   ]\n",
            "deep: [ 0.31445   1.2024    0.066651 -0.20096  -0.049636  0.66882  -0.049386\n",
            "  0.44174   0.1799   -0.10196  -0.43674   0.12076  -0.12495   0.43378\n",
            " -0.87784   0.010281  0.54592  -0.28928  -0.46115  -0.32058  -0.69094\n",
            "  0.49733   0.40657  -0.90062   0.69699  -1.1536   -0.12229   1.0657\n",
            "  0.93207   0.20439   3.3004    0.14223   0.46493   0.075359 -0.56755\n",
            "  0.30769  -1.1251   -0.37871   0.57479  -0.12629   0.13589   0.10633\n",
            "  0.058432  0.40321   0.10243   0.12004   0.41383   0.051987 -0.5835\n",
            " -1.1159  ]\n",
            "learning: [ 0.20461   0.48659  -0.55308  -0.27019   0.26336   0.15751  -0.28994\n",
            " -0.51824   0.051829  0.36225   0.37077   0.1322   -0.061377 -0.53606\n",
            " -0.34733  -0.043981 -0.086744  0.78305   0.41422   0.027996  0.23433\n",
            "  0.98844  -0.41049   0.6206    1.3966   -0.65427  -0.18221  -1.0293\n",
            " -0.014741 -0.25384   3.227     0.39509  -0.33042  -1.229     0.29048\n",
            "  0.33654  -0.24817   0.47105   0.32964   0.23997   0.088302 -0.91779\n",
            " -0.36671   0.9926    0.2185   -0.316     1.203     0.2699   -0.14093\n",
            "  0.70785 ]\n",
            "and: [ 0.26818   0.14346  -0.27877   0.016257  0.11384   0.69923  -0.51332\n",
            " -0.47368  -0.33075  -0.13834   0.2702    0.30938  -0.45012  -0.4127\n",
            " -0.09932   0.038085  0.029749  0.10076  -0.25058  -0.51818   0.34558\n",
            "  0.44922   0.48791  -0.080866 -0.10121  -1.3777   -0.10866  -0.23201\n",
            "  0.012839 -0.46508   3.8463    0.31362   0.13643  -0.52244   0.3302\n",
            "  0.33707  -0.35601   0.32431   0.12041   0.3512   -0.069043  0.36885\n",
            "  0.25168  -0.24517   0.25381   0.1367   -0.31178  -0.6321   -0.25028\n",
            " -0.38097 ]\n",
            "text: [ 0.32615    0.36686   -0.0074905 -0.37553    0.66715    0.21646\n",
            " -0.19801   -1.1001    -0.42221    0.10574   -0.31292    0.50953\n",
            "  0.55775    0.12019    0.31441   -0.25043   -1.0637    -1.3213\n",
            "  0.87798   -0.24627    0.27379   -0.51092    0.49324    0.52243\n",
            "  1.1636    -0.75323   -0.48053   -0.11259   -0.54595   -0.83921\n",
            "  2.9825    -1.1916    -0.51958   -0.39365   -0.1419    -0.026977\n",
            "  0.66296    0.16574   -1.1681     0.14443    1.6305    -0.17216\n",
            " -0.17436   -0.01049   -0.17794    0.93076    1.0381     0.94266\n",
            " -0.14805   -0.61109  ]\n",
            "mining: [-0.40025   -0.40659    0.0077521  1.6565    -0.83026    0.15253\n",
            " -0.83816    0.32834    0.44981    0.22079    0.62574   -0.13907\n",
            "  0.26711   -0.57841   -0.52047    0.22798    1.2644     0.67962\n",
            " -0.31575   -0.61989    1.5077    -0.17723   -0.5155    -0.99634\n",
            " -0.19396   -0.4598    -0.71236   -0.61281    0.62343    0.75497\n",
            "  2.4515    -1.6625     0.070953   0.094319   0.40127   -0.43486\n",
            " -1.2432    -0.073509   1.1299     0.42682   -0.90602   -0.60423\n",
            "  1.0386    -0.34647    0.10747   -0.46145   -0.93874    0.068166\n",
            "  0.48418   -1.0277   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print for just deep and text\n",
        "print(\"Word Embeddings:\")\n",
        "for word in ['deep', 'text']:\n",
        "  embedding = get_word_embedding(word)\n",
        "  if embedding is not None:\n",
        "    print(f\"{word}: {embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w54qqRV_DE5s",
        "outputId": "dcf47559-dee3-465c-ac53-b982e6e82687"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embeddings:\n",
            "deep: [ 0.31445   1.2024    0.066651 -0.20096  -0.049636  0.66882  -0.049386\n",
            "  0.44174   0.1799   -0.10196  -0.43674   0.12076  -0.12495   0.43378\n",
            " -0.87784   0.010281  0.54592  -0.28928  -0.46115  -0.32058  -0.69094\n",
            "  0.49733   0.40657  -0.90062   0.69699  -1.1536   -0.12229   1.0657\n",
            "  0.93207   0.20439   3.3004    0.14223   0.46493   0.075359 -0.56755\n",
            "  0.30769  -1.1251   -0.37871   0.57479  -0.12629   0.13589   0.10633\n",
            "  0.058432  0.40321   0.10243   0.12004   0.41383   0.051987 -0.5835\n",
            " -1.1159  ]\n",
            "text: [ 0.32615    0.36686   -0.0074905 -0.37553    0.66715    0.21646\n",
            " -0.19801   -1.1001    -0.42221    0.10574   -0.31292    0.50953\n",
            "  0.55775    0.12019    0.31441   -0.25043   -1.0637    -1.3213\n",
            "  0.87798   -0.24627    0.27379   -0.51092    0.49324    0.52243\n",
            "  1.1636    -0.75323   -0.48053   -0.11259   -0.54595   -0.83921\n",
            "  2.9825    -1.1916    -0.51958   -0.39365   -0.1419    -0.026977\n",
            "  0.66296    0.16574   -1.1681     0.14443    1.6305    -0.17216\n",
            " -0.17436   -0.01049   -0.17794    0.93076    1.0381     0.94266\n",
            " -0.14805   -0.61109  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#Problem 2 - Book Question"
      ],
      "metadata": {
        "id": "2d1RZ1Y7Ci0H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1iJvHjbaQeb",
        "outputId": "a7c837c7-36d1-4e43-b8ee-2440f46c2c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking directory: /content/drive/MyDrive/KDD/Homework10/novels_by_hgwells\n",
            "Files in directory: ['The Country of the Blind, and Other Stories.txt', 'The War of the Worlds.txt', 'The Time Machine.txt', 'The Door in the Wall, and Other Stories.txt', 'The Island of Doctor Moreau.txt']\n",
            "Processing file: The Country of the Blind, and Other Stories.txt\n",
            "Found start and end markers in The Country of the Blind, and Other Stories.txt\n",
            "Extracted 10110 sentences from The Country of the Blind, and Other Stories.txt\n",
            "Processing file: The War of the Worlds.txt\n",
            "Found start and end markers in The War of the Worlds.txt\n",
            "Extracted 3168 sentences from The War of the Worlds.txt\n",
            "Processing file: The Time Machine.txt\n",
            "Found start and end markers in The Time Machine.txt\n",
            "Extracted 1824 sentences from The Time Machine.txt\n",
            "Processing file: The Door in the Wall, and Other Stories.txt\n",
            "Found start and end markers in The Door in the Wall, and Other Stories.txt\n",
            "Extracted 2490 sentences from The Door in the Wall, and Other Stories.txt\n",
            "Processing file: The Island of Doctor Moreau.txt\n",
            "Found start and end markers in The Island of Doctor Moreau.txt\n",
            "Extracted 2570 sentences from The Island of Doctor Moreau.txt\n",
            "Total sentences in corpus: 20162\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "corpus = []\n",
        "# Updated path to include the subdirectory\n",
        "path = '/content/drive/MyDrive/KDD/Homework10/novels_by_hgwells'\n",
        "print(f\"Checking directory: {path}\")\n",
        "print(f\"Files in directory: {os.listdir(path)}\") # Added print statement here\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith('.txt'):\n",
        "        print(f\"Processing file: {filename}\")\n",
        "        with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "            # Extract main content using start/end markers\n",
        "            start_pattern = re.compile(r'\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK [^*]+ \\*\\*\\*')\n",
        "            end_pattern = re.compile(r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [^*]+ \\*\\*\\*')\n",
        "            start_match = start_pattern.search(text)\n",
        "            end_match = end_pattern.search(text)\n",
        "\n",
        "            if start_match and end_match:\n",
        "                print(f\"Found start and end markers in {filename}\")\n",
        "                text = text[start_match.end():end_match.start()]\n",
        "                sentences = sent_tokenize(text)\n",
        "                print(f\"Extracted {len(sentences)} sentences from {filename}\")\n",
        "                corpus.extend(sentences)\n",
        "            else:\n",
        "                print(f\"Start or end markers not found in {filename}\")\n",
        "\n",
        "print(f\"Total sentences in corpus: {len(corpus)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FyDNf02FUa2n"
      },
      "outputs": [],
      "source": [
        "# Preprocess each sentence (lowercase, remove punctuation)\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
        "    tokens = sentence.split()\n",
        "    return tokens # Return empty list if no tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_ZgzZREmUeIt"
      },
      "outputs": [],
      "source": [
        "preprocessed_corpus = []\n",
        "for sentence in corpus:\n",
        "    tokens = preprocess_sentence(sentence)\n",
        "    if tokens:\n",
        "        preprocessed_corpus.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fER3xzJdUg2p",
        "outputId": "6f73daed-7c29-4224-d43b-2c3223108bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW vocabulary size: 18798\n",
            "Epoch 0 begin\n",
            "Epoch 0 end\n",
            "Epoch 1 begin\n",
            "Epoch 1 end\n",
            "Epoch 2 begin\n",
            "Epoch 2 end\n",
            "Epoch 3 begin\n",
            "Epoch 3 end\n",
            "Epoch 4 begin\n",
            "Epoch 4 end\n",
            "Skip-Gram vocabulary size: 18798\n",
            "Epoch 0 begin\n",
            "Epoch 0 end\n",
            "Epoch 1 begin\n",
            "Epoch 1 end\n",
            "Epoch 2 begin\n",
            "Epoch 2 end\n",
            "Epoch 3 begin\n",
            "Epoch 3 end\n",
            "Epoch 4 begin\n",
            "Epoch 4 end\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1231989, 1696735)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_begin(self, model):\n",
        "        print(f\"Epoch {self.epoch} begin\")\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        print(f\"Epoch {self.epoch} end\")\n",
        "        self.epoch += 1\n",
        "\n",
        "cbow_model = Word2Vec(vector_size=50, window=5, min_count=1, sg=0)\n",
        "cbow_model.build_vocab(preprocessed_corpus)\n",
        "print(f\"CBOW vocabulary size: {len(cbow_model.wv)}\") # Added print statement\n",
        "cbow_model.train(preprocessed_corpus, epochs=5, total_examples=cbow_model.corpus_count, callbacks=[EpochLogger()])\n",
        "\n",
        "skipgram_model = Word2Vec(vector_size=50, window=5, min_count=1, sg=1)\n",
        "skipgram_model.build_vocab(preprocessed_corpus)\n",
        "print(f\"Skip-Gram vocabulary size: {len(skipgram_model.wv)}\") # Added print statement\n",
        "skipgram_model.train(preprocessed_corpus, epochs=5, total_examples=skipgram_model.corpus_count, callbacks=[EpochLogger()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDTTrwUwVduS",
        "outputId": "3784c93b-03cc-433e-b80d-a71b22ef12fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1446426931.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "glove_input_file = '/content/drive/MyDrive/KDD/Homework10/glove.6B/glove.6B.50d.txt'\n",
        "word2vec_output_file = 'glove.6B.50d.word2vec.txt'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TgnchjmSWRE-"
      },
      "outputs": [],
      "source": [
        "glove_model_custom = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MomntLwxWYUg",
        "outputId": "fc7f3432-b2cc-415f-bff6-6b56fe5b2e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Embeddings for 'soldiers' ---\n",
            "CBOW: [ 0.00428076  0.01607017 -0.02445038  0.09522677 -0.09397167 -0.11088915\n",
            "  0.2848532   0.31140405 -0.2820228  -0.19463728 -0.06633068 -0.31348386\n",
            "  0.0306413  -0.03870173 -0.00333034  0.05831794  0.3060176   0.3065402\n",
            " -0.35382578 -0.26691005  0.07930589  0.3663335   0.35946384  0.10097729\n",
            "  0.14549662  0.12186701 -0.10527173  0.09954178 -0.18767412  0.09300762\n",
            " -0.05999088 -0.15066198  0.3319415  -0.0229233  -0.09732552 -0.03613795\n",
            "  0.26741144 -0.01274554 -0.13827142 -0.19983621  0.25736588 -0.13391352\n",
            "  0.01602205  0.07153047  0.31281388 -0.02448955  0.02949955 -0.12181917\n",
            "  0.25139582  0.30418816]\n",
            "Skip-Gram: [-0.11550249  0.22462265 -0.06911512  0.01580437 -0.03942475 -0.14783873\n",
            "  0.4139873   0.3983046  -0.32739916 -0.27414677  0.05632718 -0.41676018\n",
            "  0.16751939  0.15837559  0.14310604  0.09334166  0.34733245  0.41401324\n",
            " -0.380431   -0.34294978  0.36649477  0.4372385   0.50278217  0.18784182\n",
            "  0.13550106 -0.01749332 -0.05557569  0.04227084 -0.08222181  0.11726474\n",
            " -0.14010364 -0.4763      0.31675467 -0.03169748 -0.01809075  0.07514539\n",
            "  0.44404712  0.01448465 -0.09612224 -0.2752097   0.46284932 -0.04680844\n",
            "  0.07435112  0.00441519  0.3173498   0.15870307  0.09165769 -0.10444186\n",
            "  0.24602747  0.32433113]\n",
            "GloVe: [ 7.2804e-01 -9.0658e-01  7.6148e-01 -1.1923e+00  9.8958e-01 -6.0667e-01\n",
            " -5.4329e-02  8.5521e-01 -5.3650e-01 -1.9294e+00 -1.2303e-01 -1.2263e+00\n",
            " -1.9933e-03 -6.0596e-01  1.4671e-01 -8.3154e-01 -2.0753e-01  4.1731e-01\n",
            " -6.8892e-01 -3.7915e-01 -5.3101e-01  1.1372e+00  5.7965e-01 -2.5881e-01\n",
            " -4.8883e-01 -1.5362e+00 -3.7883e-01 -4.8205e-01  5.0707e-01  5.0248e-02\n",
            "  3.0791e+00  1.9022e-01 -8.0112e-01 -8.5541e-02  4.2080e-01  1.4798e+00\n",
            "  1.8674e-01 -1.1900e+00 -6.8793e-01  4.7058e-01 -7.5919e-02  2.1853e-01\n",
            "  1.0405e+00 -5.6027e-01  8.7449e-01 -1.0226e+00 -8.1857e-01 -1.4132e-01\n",
            " -7.4057e-01 -9.6798e-01]\n"
          ]
        }
      ],
      "source": [
        "cbow_soldiers = cbow_model.wv['soldiers']\n",
        "skipgram_soldiers = skipgram_model.wv['soldiers']\n",
        "glove_soldiers = glove_model_custom['soldiers'] if 'soldiers' in glove_model_custom else np.zeros(50)\n",
        "\n",
        "print(\"\\n--- Embeddings for 'soldiers' ---\")\n",
        "print(\"CBOW:\", cbow_soldiers)\n",
        "print(\"Skip-Gram:\", skipgram_soldiers)\n",
        "print(\"GloVe:\", glove_soldiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sishq3r0WdI8",
        "outputId": "1b822146-2f2f-4791-8788-ee14afc22011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top 10 Words Similar to 'road' ---\n",
            "CBOW: [('water', 0.9952878952026367), ('shadow', 0.9942275285720825), ('bushes', 0.9938081502914429), ('enclosure', 0.9931560754776001), ('darkness', 0.9927343130111694), ('kitchen', 0.9920802712440491), ('bridge', 0.9918335676193237), ('floor', 0.9915972948074341), ('lane', 0.9910646080970764), ('corner', 0.9905678629875183)]\n",
            "Skip-Gram: [('lane', 0.962975800037384), ('river', 0.9563546180725098), ('edge', 0.9563457369804382), ('slope', 0.9547950029373169), ('bushes', 0.9540297389030457), ('park', 0.9528161287307739), ('street', 0.9517647624015808), ('station', 0.9480806589126587), ('stream', 0.9473429322242737), ('opposite', 0.9440400004386902)]\n",
            "GloVe: [('bridge', 0.8527506589889526), ('highway', 0.8253951072692871), ('route', 0.8184633255004883), ('lane', 0.8131610751152039), ('junction', 0.8032940030097961), ('roads', 0.7938767075538635), ('along', 0.779608428478241), ('west', 0.7775814533233643), ('intersection', 0.772043764591217), ('park', 0.7659005522727966)]\n"
          ]
        }
      ],
      "source": [
        "# Top 10 similar words for \"road\"\n",
        "cbow_similar_road = cbow_model.wv.most_similar('road', topn=10)\n",
        "skipgram_similar_road = skipgram_model.wv.most_similar('road', topn=10)\n",
        "glove_similar_road = glove_model_custom.most_similar('road', topn=10) if 'road' in glove_model_custom else []\n",
        "\n",
        "print(\"\\n--- Top 10 Words Similar to 'road' ---\")\n",
        "print(\"CBOW:\", cbow_similar_road)\n",
        "print(\"Skip-Gram:\", skipgram_similar_road)\n",
        "print(\"GloVe:\", glove_similar_road)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}